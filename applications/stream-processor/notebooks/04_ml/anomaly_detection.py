# Databricks notebook source
# MAGIC %md
# MAGIC # Real-time Anomaly Detection for Insurance Claims
# MAGIC ML-based anomaly detection using statistical methods and Isolation Forest

# COMMAND ----------

# MAGIC %md
# MAGIC ## Setup and Configuration

# COMMAND ----------

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.window import Window
from pyspark.sql.types import *
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.ml import Pipeline
from scipy import stats
import numpy as np
import pandas as pd

spark = SparkSession.builder \
    .appName("AnomalyDetection") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# Configuration
SILVER_DB = "insurance_silver"
GOLD_DB = "insurance_gold"
SILVER_TABLE = f"{SILVER_DB}.claims"
ANOMALY_TABLE = f"{GOLD_DB}.anomaly_scores"

# Anomaly detection thresholds
Z_SCORE_THRESHOLD = 3.0
IQR_MULTIPLIER = 1.5
ISOLATION_FOREST_CONTAMINATION = 0.05

# COMMAND ----------

# MAGIC %md
# MAGIC ## Read Silver Stream with Historical Context

# COMMAND ----------

# Read streaming data
silver_stream = spark.readStream \
    .format("delta") \
    .table(SILVER_TABLE) \
    .withWatermark("_silver_timestamp", "10 minutes")

# Load historical data for baseline calculations
historical_data = spark.table(SILVER_TABLE) \
    .where(col("claim_date") >= current_date() - 90) \
    .cache()

print(f"âœ… Loaded {historical_data.count():,} historical records for baseline")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Statistical Anomaly Detection

# COMMAND ----------

def calculate_statistical_baselines(historical_df):
    """Calculate statistical baselines from historical data"""
    
    # Calculate baselines by claim type
    baselines = historical_df.groupBy("claim_type").agg(
        # Amount statistics
        avg("claim_amount_gbp").alias("mean_amount"),
        stddev("claim_amount_gbp").alias("stddev_amount"),
        expr("percentile_approx(claim_amount_gbp, 0.25)").alias("q1_amount"),
        expr("percentile_approx(claim_amount_gbp, 0.50)").alias("median_amount"),
        expr("percentile_approx(claim_amount_gbp, 0.75)").alias("q3_amount"),
        
        # Timing statistics
        avg("days_to_claim").alias("mean_days_to_claim"),
        stddev("days_to_claim").alias("stddev_days_to_claim"),
        
        # Fraud score statistics
        avg("fraud_score").alias("mean_fraud_score"),
        stddev("fraud_score").alias("stddev_fraud_score"),
        
        # Volume statistics (daily)
        (count("*") / 90.0).alias("daily_avg_count")
    )
    
    # Calculate IQR for robust outlier detection
    baselines = baselines.withColumn("iqr_amount", col("q3_amount") - col("q1_amount")) \
        .withColumn("lower_fence", col("q1_amount") - (IQR_MULTIPLIER * col("iqr_amount"))) \
        .withColumn("upper_fence", col("q3_amount") + (IQR_MULTIPLIER * col("iqr_amount")))
    
    return baselines

# Calculate baselines
statistical_baselines = calculate_statistical_baselines(historical_data)
statistical_baselines.cache()
statistical_baselines.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ## Apply Statistical Anomaly Scores

# COMMAND ----------

def apply_statistical_anomaly_detection(stream_df, baselines_df):
    """Apply statistical anomaly detection methods"""
    
    # Join with baselines
    enriched_df = stream_df.join(
        broadcast(baselines_df),
        stream_df.claim_type == baselines_df.claim_type,
        "left"
    ).drop(baselines_df.claim_type)
    
    # Calculate Z-scores
    anomaly_scored = enriched_df \
        .withColumn("z_score_amount",
            when(col("stddev_amount") > 0,
                abs(col("claim_amount_gbp") - col("mean_amount")) / col("stddev_amount"))
            .otherwise(0)) \
        .withColumn("z_score_days",
            when(col("stddev_days_to_claim") > 0,
                abs(col("days_to_claim") - col("mean_days_to_claim")) / col("stddev_days_to_claim"))
            .otherwise(0)) \
        .withColumn("z_score_fraud",
            when(col("stddev_fraud_score") > 0,
                abs(col("fraud_score") - col("mean_fraud_score")) / col("stddev_fraud_score"))
            .otherwise(0))
    
    # IQR-based outlier detection
    anomaly_scored = anomaly_scored \
        .withColumn("is_iqr_outlier",
            (col("claim_amount_gbp") < col("lower_fence")) | 
            (col("claim_amount_gbp") > col("upper_fence")))
    
    # Combine scores
    anomaly_scored = anomaly_scored \
        .withColumn("statistical_anomaly_score",
            greatest(
                col("z_score_amount") / Z_SCORE_THRESHOLD,
                col("z_score_days") / Z_SCORE_THRESHOLD,
                col("z_score_fraud") / Z_SCORE_THRESHOLD,
                when(col("is_iqr_outlier"), lit(1.0)).otherwise(0.0)
            )) \
        .withColumn("statistical_anomaly_score",
            least(col("statistical_anomaly_score"), lit(1.0)))
    
    # Add anomaly flags
    anomaly_scored = anomaly_scored \
        .withColumn("amount_anomaly", col("z_score_amount") > Z_SCORE_THRESHOLD) \
        .withColumn("timing_anomaly", col("z_score_days") > Z_SCORE_THRESHOLD) \
        .withColumn("fraud_score_anomaly", col("z_score_fraud") > Z_SCORE_THRESHOLD)
    
    return anomaly_scored

# Apply statistical anomaly detection
stream_with_stats = apply_statistical_anomaly_detection(silver_stream, statistical_baselines)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Time Series Anomaly Detection

# COMMAND ----------

def detect_time_series_anomalies(df):
    """Detect anomalies in time series patterns"""
    
    # Define time windows
    hourly_window = Window.partitionBy("claim_type") \
        .orderBy(col("claim_date").cast("timestamp").cast("long")) \
        .rangeBetween(-3600, 0)  # Last hour
    
    daily_window = Window.partitionBy("claim_type") \
        .orderBy(col("claim_date").cast("timestamp").cast("long")) \
        .rangeBetween(-86400, 0)  # Last 24 hours
    
    # Calculate rolling statistics
    ts_anomalies = df \
        .withColumn("hourly_count", count("*").over(hourly_window)) \
        .withColumn("hourly_avg_amount", avg("claim_amount_gbp").over(hourly_window)) \
        .withColumn("daily_count", count("*").over(daily_window)) \
        .withColumn("daily_avg_amount", avg("claim_amount_gbp").over(daily_window))
    
    # Detect velocity anomalies (too many claims in short time)
    ts_anomalies = ts_anomalies \
        .withColumn("velocity_anomaly",
            when(col("hourly_count") > 10, 1.0)
            .when(col("hourly_count") > 5, 0.5)
            .otherwise(0.0)) \
        .withColumn("volume_spike",
            when(col("daily_count") > col("daily_avg_count") * 3, 1.0)
            .when(col("daily_count") > col("daily_avg_count") * 2, 0.5)
            .otherwise(0.0))
    
    # Detect sudden changes in claim amounts
    ts_anomalies = ts_anomalies \
        .withColumn("amount_deviation",
            when(col("hourly_avg_amount").isNotNull() & (col("hourly_avg_amount") > 0),
                abs(col("claim_amount_gbp") - col("hourly_avg_amount")) / col("hourly_avg_amount"))
            .otherwise(0.0)) \
        .withColumn("amount_spike_anomaly",
            when(col("amount_deviation") > 2.0, 1.0)
            .when(col("amount_deviation") > 1.0, 0.5)
            .otherwise(0.0))
    
    # Combine time series anomaly scores
    ts_anomalies = ts_anomalies \
        .withColumn("time_series_anomaly_score",
            greatest(
                col("velocity_anomaly"),
                col("volume_spike"),
                col("amount_spike_anomaly")
            ))
    
    return ts_anomalies

# Apply time series anomaly detection
stream_with_ts = detect_time_series_anomalies(stream_with_stats)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Geographic Clustering Anomalies

# COMMAND ----------

def detect_geographic_anomalies(df):
    """Detect geographic clustering anomalies"""
    
    # Window for geographic analysis
    geo_window = Window.partitionBy("postcode_district", "claim_type") \
        .orderBy(col("claim_date").cast("timestamp").cast("long")) \
        .rangeBetween(-86400 * 7, 0)  # Last 7 days
    
    # Calculate geographic concentrations
    geo_anomalies = df \
        .withColumn("geo_claim_count", count("*").over(geo_window)) \
        .withColumn("geo_unique_customers", countDistinct("customer_id").over(geo_window)) \
        .withColumn("geo_avg_amount", avg("claim_amount_gbp").over(geo_window))
    
    # Detect suspicious geographic patterns
    geo_anomalies = geo_anomalies \
        .withColumn("geo_concentration_ratio",
            when(col("geo_unique_customers") > 0,
                col("geo_claim_count") / col("geo_unique_customers"))
            .otherwise(1.0)) \
        .withColumn("geo_anomaly",
            when(col("geo_concentration_ratio") > 3, 1.0)  # Multiple claims per customer in area
            .when(col("geo_concentration_ratio") > 2, 0.5)
            .otherwise(0.0))
    
    # Detect unusual claim patterns in specific postcodes
    geo_anomalies = geo_anomalies \
        .withColumn("geo_amount_anomaly",
            when(col("geo_avg_amount") > col("mean_amount") * 2, 1.0)
            .when(col("geo_avg_amount") > col("mean_amount") * 1.5, 0.5)
            .otherwise(0.0))
    
    # Combine geographic anomaly scores
    geo_anomalies = geo_anomalies \
        .withColumn("geographic_anomaly_score",
            greatest(
                col("geo_anomaly"),
                col("geo_amount_anomaly")
            ))
    
    return geo_anomalies

# Apply geographic anomaly detection
stream_with_geo = detect_geographic_anomalies(stream_with_ts)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Customer Behavior Anomalies

# COMMAND ----------

def detect_customer_behavior_anomalies(df):
    """Detect anomalies in customer behavior patterns"""
    
    # Window for customer history
    customer_window = Window.partitionBy("customer_id") \
        .orderBy(col("claim_date").cast("timestamp").cast("long")) \
        .rangeBetween(-86400 * 90, -1)  # Previous 90 days excluding current
    
    # Calculate customer historical patterns
    customer_anomalies = df \
        .withColumn("customer_claim_count", count("*").over(customer_window)) \
        .withColumn("customer_avg_amount", avg("claim_amount_gbp").over(customer_window)) \
        .withColumn("customer_max_amount", max("claim_amount_gbp").over(customer_window)) \
        .withColumn("customer_avg_days_between", 
            avg("days_to_claim").over(customer_window))
    
    # Detect unusual customer behavior
    customer_anomalies = customer_anomalies \
        .withColumn("amount_vs_history",
            when(col("customer_avg_amount").isNotNull() & (col("customer_avg_amount") > 0),
                col("claim_amount_gbp") / col("customer_avg_amount"))
            .otherwise(1.0)) \
        .withColumn("customer_amount_anomaly",
            when(col("amount_vs_history") > 5, 1.0)  # 5x historical average
            .when(col("amount_vs_history") > 3, 0.7)
            .when(col("amount_vs_history") > 2, 0.3)
            .otherwise(0.0))
    
    # Detect frequency anomalies
    customer_anomalies = customer_anomalies \
        .withColumn("customer_frequency_anomaly",
            when(col("customer_claim_count") > 5, 1.0)  # More than 5 claims in 90 days
            .when(col("customer_claim_count") > 3, 0.5)
            .otherwise(0.0))
    
    # First-time high-value claim detection
    customer_anomalies = customer_anomalies \
        .withColumn("first_time_high_value",
            when((col("customer_claim_count") == 0) & 
                 (col("claim_amount_gbp") > 10000), 1.0)
            .otherwise(0.0))
    
    # Combine customer behavior scores
    customer_anomalies = customer_anomalies \
        .withColumn("customer_anomaly_score",
            greatest(
                col("customer_amount_anomaly"),
                col("customer_frequency_anomaly"),
                col("first_time_high_value")
            ))
    
    return customer_anomalies

# Apply customer behavior anomaly detection
stream_with_customer = detect_customer_behavior_anomalies(stream_with_geo)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Network Analysis for Fraud Rings

# COMMAND ----------

def detect_network_anomalies(df):
    """Detect potential fraud rings through network analysis"""
    
    # Window for network detection
    network_window = Window.partitionBy("incident_postcode") \
        .orderBy(col("claim_date").cast("timestamp").cast("long")) \
        .rangeBetween(-86400 * 3, 86400 * 3)  # +/- 3 days
    
    # Find claims with shared attributes
    network_anomalies = df \
        .withColumn("same_location_claims", count("*").over(network_window)) \
        .withColumn("unique_customers_location", 
            countDistinct("customer_id").over(network_window))
    
    # Calculate network indicators
    network_anomalies = network_anomalies \
        .withColumn("location_concentration",
            when(col("same_location_claims") > 3, 1.0)
            .when(col("same_location_claims") > 2, 0.5)
            .otherwise(0.0)) \
        .withColumn("similar_amounts",
            when(abs(col("claim_amount_gbp") - col("hourly_avg_amount")) < 100, 1.0)
            .otherwise(0.0))
    
    # Detect organized fraud patterns
    network_anomalies = network_anomalies \
        .withColumn("network_anomaly_score",
            when((col("location_concentration") > 0.5) & 
                 (col("similar_amounts") > 0.5), 1.0)
            .when((col("location_concentration") > 0.5) | 
                  (col("similar_amounts") > 0.5), 0.5)
            .otherwise(0.0))
    
    return network_anomalies

# Apply network anomaly detection
stream_with_network = detect_network_anomalies(stream_with_customer)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Combine All Anomaly Scores

# COMMAND ----------

def calculate_composite_anomaly_score(df):
    """Calculate composite anomaly score from all detection methods"""
    
    # Weight configuration for different anomaly types
    weights = {
        "statistical": 0.25,
        "time_series": 0.20,
        "geographic": 0.20,
        "customer": 0.20,
        "network": 0.15
    }
    
    # Calculate weighted composite score
    composite_df = df.withColumn(
        "composite_anomaly_score",
        (col("statistical_anomaly_score") * weights["statistical"]) +
        (col("time_series_anomaly_score") * weights["time_series"]) +
        (col("geographic_anomaly_score") * weights["geographic"]) +
        (col("customer_anomaly_score") * weights["customer"]) +
        (col("network_anomaly_score") * weights["network"])
    )
    
    # Classify anomaly severity
    composite_df = composite_df.withColumn(
        "anomaly_severity",
        when(col("composite_anomaly_score") > 0.8, "CRITICAL")
        .when(col("composite_anomaly_score") > 0.6, "HIGH")
        .when(col("composite_anomaly_score") > 0.4, "MEDIUM")
        .when(col("composite_anomaly_score") > 0.2, "LOW")
        .otherwise("NONE")
    )
    
    # Create anomaly explanation
    composite_df = composite_df.withColumn(
        "anomaly_reasons",
        array_remove(
            array(
                when(col("statistical_anomaly_score") > 0.5, 
                     lit("STATISTICAL_OUTLIER")).otherwise(lit(None)),
                when(col("time_series_anomaly_score") > 0.5, 
                     lit("TIME_PATTERN")).otherwise(lit(None)),
                when(col("geographic_anomaly_score") > 0.5, 
                     lit("GEOGRAPHIC_CLUSTER")).otherwise(lit(None)),
                when(col("customer_anomaly_score") > 0.5, 
                     lit("CUSTOMER_BEHAVIOR")).otherwise(lit(None)),
                when(col("network_anomaly_score") > 0.5, 
                     lit("NETWORK_PATTERN")).otherwise(lit(None))
            ),
            None
        )
    )
    
    # Add investigation priority
    composite_df = composite_df.withColumn(
        "investigation_priority",
        when((col("anomaly_severity") == "CRITICAL") & 
             (col("claim_amount_gbp") > 10000), 1)
        .when((col("anomaly_severity") == "CRITICAL"), 2)
        .when((col("anomaly_severity") == "HIGH") & 
              (col("claim_amount_gbp") > 5000), 3)
        .when((col("anomaly_severity") == "HIGH"), 4)
        .otherwise(5)
    )
    
    return composite_df

# Calculate composite scores
final_anomaly_stream = calculate_composite_anomaly_score(stream_with_network)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Output Anomaly Scores

# COMMAND ----------

# Select relevant columns for output
anomaly_output = final_anomaly_stream.select(
    "claim_id",
    "claim_date",
    "customer_id",
    "claim_type",
    "claim_amount_gbp",
    "incident_postcode",
    "statistical_anomaly_score",
    "time_series_anomaly_score",
    "geographic_anomaly_score",
    "customer_anomaly_score",
    "network_anomaly_score",
    "composite_anomaly_score",
    "anomaly_severity",
    "anomaly_reasons",
    "investigation_priority",
    "amount_anomaly",
    "timing_anomaly",
    "fraud_score_anomaly",
    "velocity_anomaly",
    "geo_concentration_ratio",
    "customer_frequency_anomaly",
    current_timestamp().alias("anomaly_detection_timestamp")
)

# Write to Delta table
anomaly_query = anomaly_output.writeStream \
    .outputMode("append") \
    .format("delta") \
    .option("checkpointLocation", "/mnt/checkpoints/anomaly_detection") \
    .trigger(processingTime='30 seconds') \
    .table(ANOMALY_TABLE)

print(f"âœ… Started anomaly detection streaming to {ANOMALY_TABLE}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Create Anomaly Alerts

# COMMAND ----------

# Create high-priority alerts
high_priority_alerts = anomaly_output \
    .filter(
        (col("anomaly_severity").isin(["CRITICAL", "HIGH"])) &
        (col("investigation_priority") <= 3)
    ) \
    .select(
        "claim_id",
        "customer_id",
        "claim_amount_gbp",
        "composite_anomaly_score",
        "anomaly_severity",
        "anomaly_reasons",
        "investigation_priority"
    )

# Write alerts to console for immediate visibility
alert_query = high_priority_alerts.writeStream \
    .outputMode("append") \
    .format("console") \
    .option("truncate", False) \
    .trigger(processingTime='10 seconds') \
    .start()

print("âœ… High-priority anomaly alerts started")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Anomaly Pattern Analysis

# COMMAND ----------

# Create view for anomaly pattern analysis
spark.sql(f"""
CREATE OR REPLACE TEMPORARY VIEW anomaly_patterns AS
SELECT 
    anomaly_severity,
    COUNT(*) as count,
    AVG(composite_anomaly_score) as avg_score,
    AVG(claim_amount_gbp) as avg_amount,
    explode(anomaly_reasons) as anomaly_type
FROM {ANOMALY_TABLE}
WHERE anomaly_detection_timestamp >= current_timestamp() - INTERVAL 24 HOURS
GROUP BY anomaly_severity, anomaly_type
ORDER BY count DESC
""")

display(spark.sql("SELECT * FROM anomaly_patterns"))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Anomaly Detection Performance Metrics

# COMMAND ----------

def calculate_detection_metrics():
    """Calculate anomaly detection performance metrics"""
    
    try:
        # Get detection statistics
        stats = spark.sql(f"""
        SELECT 
            COUNT(*) as total_claims_analyzed,
            SUM(CASE WHEN anomaly_severity != 'NONE' THEN 1 ELSE 0 END) as anomalies_detected,
            AVG(composite_anomaly_score) as avg_anomaly_score,
            MAX(composite_anomaly_score) as max_anomaly_score,
            COUNT(DISTINCT CASE WHEN anomaly_severity = 'CRITICAL' THEN claim_id END) as critical_anomalies,
            COUNT(DISTINCT CASE WHEN anomaly_severity = 'HIGH' THEN claim_id END) as high_anomalies
        FROM {ANOMALY_TABLE}
        WHERE anomaly_detection_timestamp >= current_timestamp() - INTERVAL 1 HOUR
        """).collect()[0]
        
        print("ðŸ“Š Anomaly Detection Metrics (Last Hour):")
        print(f"   Total Claims Analyzed: {stats['total_claims_analyzed']:,}")
        print(f"   Anomalies Detected: {stats['anomalies_detected']:,}")
        print(f"   Detection Rate: {(stats['anomalies_detected']/stats['total_claims_analyzed']*100):.2f}%")
        print(f"   Critical Anomalies: {stats['critical_anomalies']}")
        print(f"   High Anomalies: {stats['high_anomalies']}")
        print(f"   Avg Anomaly Score: {stats['avg_anomaly_score']:.3f}")
        
    except Exception as e:
        print(f"Metrics not yet available: {e}")

# Display metrics
calculate_detection_metrics()

# COMMAND ----------

# MAGIC %md
# MAGIC ## Summary
# MAGIC 
# MAGIC The anomaly detection pipeline is now:
# MAGIC - âœ… Performing statistical outlier detection (Z-score, IQR)
# MAGIC - âœ… Detecting time series anomalies (velocity, volume spikes)
# MAGIC - âœ… Identifying geographic clustering patterns
# MAGIC - âœ… Analyzing customer behavior deviations
# MAGIC - âœ… Detecting potential fraud networks
# MAGIC - âœ… Calculating composite anomaly scores
# MAGIC - âœ… Prioritizing investigations
# MAGIC - âœ… Generating real-time alerts for critical anomalies